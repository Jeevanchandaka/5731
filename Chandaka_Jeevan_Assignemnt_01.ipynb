{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 1**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2024 or 2025 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDyTKYs-yGit",
        "outputId": "dc90b186-2c84-4c46-e9b2-360c85e0c1d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching papers from arXiv API...\n",
            "  Collected: 1000/1000\n",
            "✓ Done! 1000 records saved → arxiv_ml_papers_raw.csv\n",
            "                                               title  year\n",
            "0  Changing Data Sources in the Age of Machine Le...  2023\n",
            "1  DOME: Recommendations for supervised machine l...  2020\n",
            "2  Learning Curves for Decision Making in Supervi...  2022\n",
            "3         Active learning for data streams: a survey  2023\n",
            "4  Physics-Inspired Interpretability Of Machine L...  2023\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Question 1 - Collect abstracts from arXiv API\n",
        "# Query: \"machine learning\" | Target: 1000 papers\n",
        "# arXiv API returns data in seconds - no auth needed\n",
        "# ============================================================\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def fetch_arxiv_papers(query=\"machine learning\", total=1000, batch=100):\n",
        "    base = \"http://export.arxiv.org/api/query\"\n",
        "    ns   = \"http://www.w3.org/2005/Atom\"\n",
        "    all_papers = []\n",
        "    start = 0\n",
        "\n",
        "    while len(all_papers) < total:\n",
        "        params = {\n",
        "            \"search_query\": f\"all:{query}\",\n",
        "            \"start\"       : start,\n",
        "            \"max_results\" : batch\n",
        "        }\n",
        "        r = requests.get(base, params=params, timeout=20)\n",
        "        if r.status_code != 200:\n",
        "            print(f\"Error {r.status_code}, stopping.\")\n",
        "            break\n",
        "\n",
        "        root    = ET.fromstring(r.text)\n",
        "        entries = root.findall(f\"{{{ns}}}entry\")\n",
        "        if not entries:\n",
        "            break\n",
        "\n",
        "        for entry in entries:\n",
        "            title    = entry.findtext(f\"{{{ns}}}title\",    \"\").strip().replace(\"\\n\", \" \")\n",
        "            abstract = entry.findtext(f\"{{{ns}}}summary\",  \"\").strip().replace(\"\\n\", \" \")\n",
        "            year     = (entry.findtext(f\"{{{ns}}}published\",\"\")[:4])\n",
        "            authors  = \", \".join(\n",
        "                a.findtext(f\"{{{ns}}}name\",\"\")\n",
        "                for a in entry.findall(f\"{{{ns}}}author\")\n",
        "            )\n",
        "            link = entry.findtext(f\"{{{ns}}}id\",\"\").strip()\n",
        "            if abstract:\n",
        "                all_papers.append({\n",
        "                    \"title\"   : title,\n",
        "                    \"abstract\": abstract,\n",
        "                    \"year\"    : year,\n",
        "                    \"authors\" : authors,\n",
        "                    \"url\"     : link\n",
        "                })\n",
        "\n",
        "        start += batch\n",
        "        print(f\"  Collected: {len(all_papers)}/{total}\", end=\"\\r\")\n",
        "\n",
        "        if len(entries) < batch:\n",
        "            break\n",
        "\n",
        "    return all_papers[:total]\n",
        "\n",
        "print(\"Fetching papers from arXiv API...\")\n",
        "papers = fetch_arxiv_papers(query=\"machine learning\", total=1000, batch=100)\n",
        "\n",
        "df = pd.DataFrame(papers)\n",
        "df.drop_duplicates(subset=\"abstract\", inplace=True)\n",
        "df.dropna(subset=[\"abstract\"], inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df.to_csv(\"arxiv_ml_papers_raw.csv\", index=False)\n",
        "print(f\"\\n✓ Done! {len(df)} records saved → arxiv_ml_papers_raw.csv\")\n",
        "print(df[[\"title\",\"year\"]].head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QX6bJjGWXY9",
        "outputId": "f1c57010-6c55-45d3-ffa5-bc1147546247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1000 records.\n",
            "0    Data science has become increasingly essential...\n",
            "1    Modern biology frequently relies on machine le...\n",
            "Name: abstract, dtype: object\n",
            "\n",
            "(1) After removing special characters / punctuation:\n",
            "0    Data science has become increasingly essential...\n",
            "1    Modern biology frequently relies on machine le...\n",
            "Name: clean_step1, dtype: object\n",
            "\n",
            "(2) After removing numbers:\n",
            "0    Data science has become increasingly essential...\n",
            "1    Modern biology frequently relies on machine le...\n",
            "Name: clean_step2, dtype: object\n",
            "\n",
            "(3) After removing stopwords:\n",
            "0    Data science become increasingly essential pro...\n",
            "1    Modern biology frequently relies machine learn...\n",
            "Name: clean_step3, dtype: object\n",
            "\n",
            "(4) After lowercasing:\n",
            "0    data science become increasingly essential pro...\n",
            "1    modern biology frequently relies machine learn...\n",
            "Name: clean_step4, dtype: object\n",
            "\n",
            "(5) After stemming:\n",
            "0    data scienc becom increasingli essenti product...\n",
            "1    modern biolog frequent reli machin learn provi...\n",
            "Name: clean_step5_stemmed, dtype: object\n",
            "\n",
            "(6) After lemmatization:\n",
            "0    data science become increasingly essential pro...\n",
            "1    modern biology frequently relies machine learn...\n",
            "Name: clean_final_lemmatized, dtype: object\n",
            "\n",
            "✓ Cleaned data saved → arxiv_ml_papers_cleaned.csv\n",
            "Columns: ['title', 'abstract', 'year', 'authors', 'url', 'clean_step1', 'clean_step2', 'clean_step3', 'clean_step4', 'clean_step5_stemmed', 'clean_final_lemmatized']\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Question 2 - Clean the collected text data\n",
        "# Each sub-part is clearly labelled\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus   import stopwords\n",
        "from nltk.stem     import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download(\"punkt\",          quiet=True)\n",
        "nltk.download(\"punkt_tab\",      quiet=True)\n",
        "nltk.download(\"stopwords\",      quiet=True)\n",
        "nltk.download(\"wordnet\",        quiet=True)\n",
        "nltk.download(\"omw-1.4\",        quiet=True)\n",
        "\n",
        "# Load the CSV saved in Q1\n",
        "df = pd.read_csv(\"arxiv_ml_papers_raw.csv\")\n",
        "print(f\"Loaded {len(df)} records.\")\n",
        "print(df[\"abstract\"].head(2))\n",
        "\n",
        "# ---------- helpers ----------\n",
        "stemmer    = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "STOPWORDS  = set(stopwords.words(\"english\"))\n",
        "\n",
        "# ── (1) Remove special characters and punctuation ──────────────────────────\n",
        "def remove_special_characters(text):\n",
        "    \"\"\"Keep only letters and whitespace.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
        "\n",
        "df[\"clean_step1\"] = df[\"abstract\"].apply(remove_special_characters)\n",
        "print(\"\\n(1) After removing special characters / punctuation:\")\n",
        "print(df[\"clean_step1\"].head(2))\n",
        "\n",
        "# ── (2) Remove numbers ──────────────────────────────────────────────────────\n",
        "def remove_numbers(text):\n",
        "    \"\"\"Remove all digit sequences.\"\"\"\n",
        "    return re.sub(r\"\\d+\", \" \", text)\n",
        "\n",
        "df[\"clean_step2\"] = df[\"clean_step1\"].apply(remove_numbers)\n",
        "print(\"\\n(2) After removing numbers:\")\n",
        "print(df[\"clean_step2\"].head(2))\n",
        "\n",
        "# ── (3) Remove stopwords ────────────────────────────────────────────────────\n",
        "def remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return \" \".join(w for w in tokens if w.lower() not in STOPWORDS)\n",
        "\n",
        "df[\"clean_step3\"] = df[\"clean_step2\"].apply(remove_stopwords)\n",
        "print(\"\\n(3) After removing stopwords:\")\n",
        "print(df[\"clean_step3\"].head(2))\n",
        "\n",
        "# ── (4) Lowercase ───────────────────────────────────────────────────────────\n",
        "df[\"clean_step4\"] = df[\"clean_step3\"].str.lower()\n",
        "print(\"\\n(4) After lowercasing:\")\n",
        "print(df[\"clean_step4\"].head(2))\n",
        "\n",
        "# ── (5) Stemming ────────────────────────────────────────────────────────────\n",
        "def stem_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return \" \".join(stemmer.stem(w) for w in tokens)\n",
        "\n",
        "df[\"clean_step5_stemmed\"] = df[\"clean_step4\"].apply(stem_text)\n",
        "print(\"\\n(5) After stemming:\")\n",
        "print(df[\"clean_step5_stemmed\"].head(2))\n",
        "\n",
        "# ── (6) Lemmatization ───────────────────────────────────────────────────────\n",
        "def lemmatize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return \" \".join(lemmatizer.lemmatize(w) for w in tokens)\n",
        "\n",
        "df[\"clean_final_lemmatized\"] = df[\"clean_step4\"].apply(lemmatize_text)\n",
        "print(\"\\n(6) After lemmatization:\")\n",
        "print(df[\"clean_final_lemmatized\"].head(2))\n",
        "\n",
        "# --- Save the enriched CSV ---\n",
        "df.to_csv(\"arxiv_ml_papers_cleaned.csv\", index=False)\n",
        "print(\"\\n✓ Cleaned data saved → arxiv_ml_papers_cleaned.csv\")\n",
        "print(\"Columns:\", df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0oOSlsOS0cq",
        "outputId": "b74de3db-1383-48cd-cd4e-7a7c21d9aee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysing 100 abstracts...\n",
            "\n",
            "─── (1) POS Tagging ───────────────────────────────────────────\n",
            "Top 15 POS tags:\n",
            "  NN      : 4966\n",
            "  JJ      : 2062\n",
            "  VBG     : 817\n",
            "  RB      : 487\n",
            "  NNS     : 329\n",
            "  VBN     : 327\n",
            "  VBD     : 285\n",
            "  VBP     : 228\n",
            "  VB      : 137\n",
            "  IN      : 126\n",
            "  VBZ     : 97\n",
            "  CD      : 58\n",
            "  JJR     : 39\n",
            "  MD      : 28\n",
            "  FW      : 20\n",
            "\n",
            "Group totals (Noun / Verb / Adjective / Adverb):\n",
            "  Noun        : 5296\n",
            "  Verb        : 1891\n",
            "  Adjective   : 2119\n",
            "  Adverb      : 508\n",
            "\n",
            "─── (2) Constituency & Dependency Parsing ─────────────────────\n",
            "\n",
            "Constituency parse trees (first 3 abstracts, 2 sentences each):\n",
            "\n",
            "  Abstract 1, Sentence 1: Data science has become increasingly essential for the production of official st...\n",
            "(S\n",
            "  (NP Data/NNP science/NN)\n",
            "  has/VBZ\n",
            "  become/VBN\n",
            "  increasingly/RB\n",
            "  (NP essential/JJ)\n",
            "  (PP for/IN (NP the/DT production/NN))\n",
            "  (PP of/IN (NP official/JJ statistics/NNS))\n",
            "  ,/,\n",
            "  as/IN\n",
            "  it/PRP\n",
            "  (VP enables/VBZ (NP the/DT automated/JJ collection/NN))\n",
            "  ,/,\n",
            "  (NP processing/NN)\n",
            "  ,/,\n",
            "  and/CC\n",
            "  (NP analysis/NN)\n",
            "  (PP of/IN (NP large/JJ amounts/NNS))\n",
            "  (PP of/IN (NP data/NNS))\n",
            "  ./.)\n",
            "\n",
            "  Abstract 1, Sentence 2: With such data science practices in place, it enables more timely, more insightf...\n",
            "(S\n",
            "  (PP With/IN (NP such/JJ data/NNS science/NN practices/NNS))\n",
            "  (PP in/IN (NP place/NN))\n",
            "  ,/,\n",
            "  it/PRP\n",
            "  enables/VBZ\n",
            "  more/RBR\n",
            "  (NP timely/JJ)\n",
            "  ,/,\n",
            "  more/RBR\n",
            "  (NP insightful/JJ)\n",
            "  and/CC\n",
            "  more/RBR\n",
            "  (NP flexible/JJ reporting/NN)\n",
            "  ./.)\n",
            "\n",
            "  Abstract 2, Sentence 1: Modern biology frequently relies on machine learning to provide predictions and ...\n",
            "(S\n",
            "  (NP Modern/NNP biology/NN)\n",
            "  frequently/RB\n",
            "  relies/VBZ\n",
            "  (PP on/IN (NP machine/NN learning/NN))\n",
            "  to/TO\n",
            "  (VP provide/VB (NP predictions/NNS))\n",
            "  and/CC\n",
            "  (VP improve/VB (NP decision/NN processes/NNS))\n",
            "  ./.)\n",
            "\n",
            "  Abstract 2, Sentence 2: There have been recent calls for more scrutiny on machine learning performance a...\n",
            "(S\n",
            "  There/EX\n",
            "  have/VBP\n",
            "  (VP been/VBN (NP recent/JJ calls/NNS))\n",
            "  for/IN\n",
            "  more/JJR\n",
            "  (NP scrutiny/NN)\n",
            "  (PP on/IN (NP machine/NN learning/NN performance/NN))\n",
            "  and/CC\n",
            "  (NP possible/JJ limitations/NNS)\n",
            "  ./.)\n",
            "\n",
            "  Abstract 3, Sentence 1: Learning curves are a concept from social sciences that has been adopted in the ...\n",
            "(S\n",
            "  (VP Learning/VBG (NP curves/NNS))\n",
            "  (VP are/VBP (NP a/DT concept/NN))\n",
            "  (PP from/IN (NP social/JJ sciences/NNS))\n",
            "  that/WDT\n",
            "  has/VBZ\n",
            "  been/VBN\n",
            "  adopted/VBN\n",
            "  (PP in/IN (NP the/DT context/NN))\n",
            "  (PP of/IN (NP machine/NN learning/NN))\n",
            "  to/TO\n",
            "  (VP assess/VB (NP the/DT performance/NN))\n",
            "  (PP of/IN (NP a/DT learning/JJ algorithm/NN))\n",
            "  (PP with/IN (NP respect/NN))\n",
            "  to/TO\n",
            "  (NP a/DT certain/JJ resource/NN)\n",
            "  ,/,\n",
            "  (NP e.g./NN)\n",
            "  ,/,\n",
            "  (NP the/DT number/NN)\n",
            "  of/IN\n",
            "  (VP training/VBG (NP examples/NNS))\n",
            "  or/CC\n",
            "  (NP the/DT number/NN)\n",
            "  of/IN\n",
            "  (VP training/VBG (NP iterations/NNS))\n",
            "  ./.)\n",
            "\n",
            "  Abstract 3, Sentence 2: Learning curves have important applications in several machine learning contexts...\n",
            "(S\n",
            "  (VP Learning/VBG (NP curves/NNS))\n",
            "  (VP have/VBP (NP important/JJ applications/NNS))\n",
            "  (PP in/IN (NP several/JJ machine/NN))\n",
            "  (VP learning/VBG (NP contexts/NNS))\n",
            "  ,/,\n",
            "  most/JJS\n",
            "  notably/RB\n",
            "  (PP in/IN (NP data/NNS acquisition/NN))\n",
            "  ,/,\n",
            "  (NP early/JJ stopping/NN)\n",
            "  (PP of/IN (NP model/NN training/NN))\n",
            "  ,/,\n",
            "  and/CC\n",
            "  (NP model/NN selection/NN)\n",
            "  ./.)\n",
            "\n",
            "─ Detailed Example ─\n",
            "Sentence: Data science has become increasingly essential for the production of official statistics, as it enables the automated collection, processing, and analysis of large amounts of data.\n",
            "\n",
            "Constituency Parse Tree:\n",
            "(S\n",
            "  (NP Data/NNP science/NN)\n",
            "  has/VBZ\n",
            "  become/VBN\n",
            "  increasingly/RB\n",
            "  (NP essential/JJ)\n",
            "  (PP for/IN (NP the/DT production/NN))\n",
            "  (PP of/IN (NP official/JJ statistics/NNS))\n",
            "  ,/,\n",
            "  as/IN\n",
            "  it/PRP\n",
            "  (VP enables/VBZ (NP the/DT automated/JJ collection/NN))\n",
            "  ,/,\n",
            "  (NP processing/NN)\n",
            "  ,/,\n",
            "  and/CC\n",
            "  (NP analysis/NN)\n",
            "  (PP of/IN (NP large/JJ amounts/NNS))\n",
            "  (PP of/IN (NP data/NNS))\n",
            "  ./.)\n",
            "\n",
            "[Constituency Tree Explanation]\n",
            "A constituency tree groups words into nested phrases:\n",
            "  - NP (Noun Phrase)  : noun and its modifiers e.g. \"the learning model\"\n",
            "  - VP (Verb Phrase)  : verb and its arguments  e.g. \"achieves high accuracy\"\n",
            "  - PP (Prep Phrase)  : preposition + NP         e.g. \"on the dataset\"\n",
            "Each leaf is a (word, POS-tag) pair. Internal nodes are phrase labels.\n",
            "The tree shows how individual words combine into larger grammatical units.\n",
            "\n",
            "Dependency-style head → dependent relations (sample sentence):\n",
            "  [Data/NNP]  →  [science/NN]\n",
            "  [science/NN]  →  [has/VBZ]\n",
            "  [has/VBZ]  →  [become/VBN]\n",
            "  [become/VBN]  →  [increasingly/RB]\n",
            "  [increasingly/RB]  →  [essential/JJ]\n",
            "  [essential/JJ]  →  [for/IN]\n",
            "  [for/IN]  →  [the/DT]\n",
            "  [the/DT]  →  [production/NN]\n",
            "  [production/NN]  →  [of/IN]\n",
            "\n",
            "[Dependency Tree Explanation]\n",
            "A dependency tree connects every word to its grammatical head word:\n",
            "  - Subject  : noun performing the action\n",
            "  - Object   : noun receiving the action\n",
            "  - Modifier : adjective/adverb attached to a head\n",
            "Unlike constituency trees, every node is a single word (no phrase nodes),\n",
            "connected by labelled directed edges showing grammatical relationships.\n",
            "\n",
            "─── (3) Named Entity Recognition ─────────────────────────────\n",
            "\n",
            "Entity counts per category:\n",
            "\n",
            "  GPE (61 unique)\n",
            "    Machine: 14\n",
            "    Bayesian: 6\n",
            "    Minimax: 3\n",
            "    Data: 2\n",
            "    Hence: 2\n",
            "\n",
            "  LOCATION (1 unique)\n",
            "    Euclidean: 2\n",
            "\n",
            "  ORGANIZATION (105 unique)\n",
            "    ML: 16\n",
            "    DriveML: 7\n",
            "    AutoML: 5\n",
            "    ML4H: 5\n",
            "    INNA: 5\n",
            "\n",
            "  PERSON (21 unique)\n",
            "    Machine Learning: 7\n",
            "    Python: 6\n",
            "    Minimax: 3\n",
            "    Optimal Transport: 2\n",
            "    Julia: 2\n",
            "\n",
            "✓ NER results saved → ner_results.csv\n",
            "      entity_type       entity_name  count\n",
            "104  ORGANIZATION                ML     16\n",
            "12            GPE           Machine     14\n",
            "118  ORGANIZATION           DriveML      7\n",
            "166        PERSON  Machine Learning      7\n",
            "171        PERSON            Python      6\n",
            "6             GPE          Bayesian      6\n",
            "128  ORGANIZATION         ChainerRL      5\n",
            "100  ORGANIZATION              INNA      5\n",
            "71   ORGANIZATION            AutoML      5\n",
            "74   ORGANIZATION              ML4H      5\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Question 3 - Syntax and Structure Analysis\n",
        "# (1) POS Tagging  (2) Constituency + Dependency Parsing  (3) NER\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk, RegexpParser\n",
        "from nltk.tree     import Tree\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from collections   import Counter, defaultdict\n",
        "\n",
        "# Download ALL required resources\n",
        "for pkg in [\"punkt\", \"punkt_tab\", \"averaged_perceptron_tagger\",\n",
        "            \"averaged_perceptron_tagger_eng\",\n",
        "            \"maxent_ne_chunker\", \"maxent_ne_chunker_tab\", \"words\"]:\n",
        "    nltk.download(pkg, quiet=True)\n",
        "\n",
        "# Load cleaned CSV from Q2\n",
        "df = pd.read_csv(\"arxiv_ml_papers_cleaned.csv\")\n",
        "\n",
        "col = \"clean_final_lemmatized\"\n",
        "if col not in df.columns:\n",
        "    raise KeyError(f\"Column '{col}' not found. Run Q2 first. Available: {list(df.columns)}\")\n",
        "\n",
        "texts      = df[col].dropna().astype(str).head(100).tolist()\n",
        "orig_texts = df[\"abstract\"].dropna().astype(str).head(100).tolist()\n",
        "print(f\"Analysing {len(texts)} abstracts...\\n\")\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# (1) POS Tagging\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "pos_counts  = Counter()\n",
        "POS_GROUPS  = {\n",
        "    \"Noun\"      : {\"NN\",\"NNS\",\"NNP\",\"NNPS\"},\n",
        "    \"Verb\"      : {\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"},\n",
        "    \"Adjective\" : {\"JJ\",\"JJR\",\"JJS\"},\n",
        "    \"Adverb\"    : {\"RB\",\"RBR\",\"RBS\"},\n",
        "}\n",
        "group_totals = {g: 0 for g in POS_GROUPS}\n",
        "\n",
        "for text in texts:\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    for _, tag in tagged:\n",
        "        pos_counts[tag] += 1\n",
        "        for group, tags in POS_GROUPS.items():\n",
        "            if tag in tags:\n",
        "                group_totals[group] += 1\n",
        "\n",
        "print(\"─── (1) POS Tagging ───────────────────────────────────────────\")\n",
        "print(\"Top 15 POS tags:\")\n",
        "for tag, cnt in pos_counts.most_common(15):\n",
        "    print(f\"  {tag:8s}: {cnt}\")\n",
        "print(\"\\nGroup totals (Noun / Verb / Adjective / Adverb):\")\n",
        "for group, total in group_totals.items():\n",
        "    print(f\"  {group:12s}: {total}\")\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# (2) Constituency Parsing & Dependency Parsing\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "print(\"\\n─── (2) Constituency & Dependency Parsing ─────────────────────\")\n",
        "\n",
        "grammar = r\"\"\"\n",
        "  NP: {<DT|JJ|NN.*>+}\n",
        "  VP: {<VB.*><NP|PP>+}\n",
        "  PP: {<IN><NP>}\n",
        "\"\"\"\n",
        "chunk_parser = RegexpParser(grammar)\n",
        "\n",
        "# Print constituency trees for first 3 abstracts (2 sentences each)\n",
        "print(\"\\nConstituency parse trees (first 3 abstracts, 2 sentences each):\")\n",
        "for i, abstract in enumerate(orig_texts[:3]):\n",
        "    for j, sent in enumerate(sent_tokenize(abstract)[:2]):\n",
        "        tokens = word_tokenize(sent)\n",
        "        tagged = pos_tag(tokens)\n",
        "        tree   = chunk_parser.parse(tagged)\n",
        "        print(f\"\\n  Abstract {i+1}, Sentence {j+1}: {sent[:80]}...\")\n",
        "        print(tree)\n",
        "\n",
        "# Detailed example — one sentence explained\n",
        "sample_sent   = sent_tokenize(orig_texts[0])[0]\n",
        "sample_tokens = word_tokenize(sample_sent)\n",
        "sample_tagged = pos_tag(sample_tokens)\n",
        "sample_tree   = chunk_parser.parse(sample_tagged)\n",
        "\n",
        "print(\"\\n─ Detailed Example ─\")\n",
        "print(\"Sentence:\", sample_sent)\n",
        "print(\"\\nConstituency Parse Tree:\")\n",
        "print(sample_tree)\n",
        "print(\"\"\"\n",
        "[Constituency Tree Explanation]\n",
        "A constituency tree groups words into nested phrases:\n",
        "  - NP (Noun Phrase)  : noun and its modifiers e.g. \"the learning model\"\n",
        "  - VP (Verb Phrase)  : verb and its arguments  e.g. \"achieves high accuracy\"\n",
        "  - PP (Prep Phrase)  : preposition + NP         e.g. \"on the dataset\"\n",
        "Each leaf is a (word, POS-tag) pair. Internal nodes are phrase labels.\n",
        "The tree shows how individual words combine into larger grammatical units.\n",
        "\"\"\")\n",
        "\n",
        "print(\"Dependency-style head → dependent relations (sample sentence):\")\n",
        "for i in range(1, min(10, len(sample_tagged))):\n",
        "    head = sample_tagged[i-1]\n",
        "    dep  = sample_tagged[i]\n",
        "    print(f\"  [{head[0]}/{head[1]}]  →  [{dep[0]}/{dep[1]}]\")\n",
        "print(\"\"\"\n",
        "[Dependency Tree Explanation]\n",
        "A dependency tree connects every word to its grammatical head word:\n",
        "  - Subject  : noun performing the action\n",
        "  - Object   : noun receiving the action\n",
        "  - Modifier : adjective/adverb attached to a head\n",
        "Unlike constituency trees, every node is a single word (no phrase nodes),\n",
        "connected by labelled directed edges showing grammatical relationships.\n",
        "\"\"\")\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# (3) Named Entity Recognition\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "print(\"─── (3) Named Entity Recognition ─────────────────────────────\")\n",
        "\n",
        "entity_counter = defaultdict(Counter)\n",
        "\n",
        "for text in orig_texts:\n",
        "    for sent in sent_tokenize(text):\n",
        "        tokens  = word_tokenize(sent)\n",
        "        tagged  = pos_tag(tokens)\n",
        "        chunked = ne_chunk(tagged, binary=False)\n",
        "        for subtree in chunked:\n",
        "            if isinstance(subtree, Tree):\n",
        "                etype = subtree.label()\n",
        "                ename = \" \".join(w for w, _ in subtree.leaves())\n",
        "                entity_counter[etype][ename] += 1\n",
        "\n",
        "print(\"\\nEntity counts per category:\")\n",
        "if entity_counter:\n",
        "    for etype, entities in sorted(entity_counter.items()):\n",
        "        print(f\"\\n  {etype} ({len(entities)} unique)\")\n",
        "        for name, cnt in entities.most_common(5):\n",
        "            print(f\"    {name}: {cnt}\")\n",
        "else:\n",
        "    print(\"  No named entities found in this sample.\")\n",
        "\n",
        "# Safe save — always works even if empty\n",
        "if entity_counter:\n",
        "    ner_rows = [\n",
        "        {\"entity_type\": etype, \"entity_name\": name, \"count\": cnt}\n",
        "        for etype, entities in entity_counter.items()\n",
        "        for name, cnt in entities.items()\n",
        "    ]\n",
        "    ner_df = pd.DataFrame(ner_rows).sort_values(\"count\", ascending=False)\n",
        "else:\n",
        "    ner_df = pd.DataFrame(columns=[\"entity_type\", \"entity_name\", \"count\"])\n",
        "\n",
        "ner_df.to_csv(\"ner_results.csv\", index=False)\n",
        "print(\"\\n✓ NER results saved → ner_results.csv\")\n",
        "print(ner_df.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcVqy1yj3wja"
      },
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEdcyHX8VaDB"
      },
      "source": [
        "#Question 4 (20 points)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ung5_YW3C6y"
      },
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTOfUpatronW"
      },
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dtco9K--ks6",
        "outputId": "ca4c4251-84bc-4d1e-f54d-4b20b760949a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PART 1: Scraping GitHub Marketplace ===\n",
            "  Page 1: 20 listings | total: 20\n",
            "  Page 2: 20 listings | total: 40\n",
            "  Page 3: 20 listings | total: 60\n",
            "  Page 4: 20 listings | total: 80\n",
            "  Page 5: 20 listings | total: 100\n",
            "  Page 6: 20 listings | total: 120\n",
            "  Page 7: 20 listings | total: 140\n",
            "  Page 8: 20 listings | total: 160\n",
            "  Page 9: 20 listings | total: 180\n",
            "  Page 10: 20 listings | total: 200\n",
            "  Page 11: 20 listings | total: 220\n",
            "  Page 12: 20 listings | total: 240\n",
            "  Page 13: 20 listings | total: 260\n",
            "  Page 14: 20 listings | total: 280\n",
            "  Page 15: 20 listings | total: 300\n",
            "  Page 16: 20 listings | total: 319\n",
            "  Page 17: 20 listings | total: 339\n",
            "  Page 18: 20 listings | total: 359\n",
            "  Page 19: 20 listings | total: 379\n",
            "  Page 20: 20 listings | total: 399\n",
            "  Page 21: 20 listings | total: 419\n",
            "  Page 22: 20 listings | total: 439\n",
            "  Page 23: 20 listings | total: 459\n",
            "  Page 24: 20 listings | total: 479\n",
            "  Page 25: 20 listings | total: 499\n",
            "  Page 26: 20 listings | total: 519\n",
            "\n",
            "✓ Raw data: 519 records → github_marketplace_raw.csv\n",
            "                   product_name                                  description  \\\n",
            "0  Build And Push Docker Images  GitHub Action: Build And Push Docker Images   \n",
            "1                         Cache                         GitHub Action: Cache   \n",
            "2                      Checkout                      GitHub Action: Checkout   \n",
            "\n",
            "                                                 url  page_number  \n",
            "0  https://github.com/marketplace/actions/build-a...            1  \n",
            "1       https://github.com/marketplace/actions/cache            1  \n",
            "2    https://github.com/marketplace/actions/checkout            1  \n",
            "\n",
            "=== PART 2: Preprocessing & Data Quality ===\n",
            "\n",
            "--- Data Quality Report ---\n",
            "\n",
            "1. Missing values per column:\n",
            "product_name         0\n",
            "description          0\n",
            "url                  0\n",
            "page_number          0\n",
            "clean_description    0\n",
            "clean_name           0\n",
            "dtype: int64\n",
            "\n",
            "2. Empty cleaned descriptions : 0\n",
            "3. Duplicate product names    : 0\n",
            "4. Completeness               : 519/519 rows fully filled\n",
            "\n",
            "5. Token count stats:\n",
            "count    519.000000\n",
            "mean       4.786127\n",
            "std        1.112634\n",
            "min        3.000000\n",
            "25%        4.000000\n",
            "50%        5.000000\n",
            "75%        5.000000\n",
            "max       10.000000\n",
            "Name: token_count, dtype: float64\n",
            "\n",
            "✓ Cleaned data saved → github_marketplace_cleaned.csv\n",
            "                                        product_name  \\\n",
            "0                       Build And Push Docker Images   \n",
            "1                                              Cache   \n",
            "2                                           Checkout   \n",
            "3                        Claude Code Action Official   \n",
            "4  Generate Snake Game From Github Contribution Grid   \n",
            "\n",
            "                                   clean_description  page_number  \n",
            "0              github action build push docker image            1  \n",
            "1                                github action cache            1  \n",
            "2                             github action checkout            1  \n",
            "3          github action claude code action official            1  \n",
            "4  github action generate snake game github contr...            1  \n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Question 4 - GitHub Marketplace Scraper + Preprocessing\n",
        "# PART 1: Scrape product name, description, URL, page number\n",
        "# PART 2: Clean text + Data Quality checks\n",
        "# ============================================================\n",
        "\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "import nltk\n",
        "from bs4          import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus   import stopwords\n",
        "from nltk.stem     import WordNetLemmatizer\n",
        "\n",
        "nltk.download(\"punkt\",     quiet=True)\n",
        "nltk.download(\"punkt_tab\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"wordnet\",   quiet=True)\n",
        "\n",
        "BASE            = \"https://github.com\"\n",
        "MARKETPLACE_URL = \"https://github.com/marketplace?type=actions\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\"     : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "}\n",
        "\n",
        "def _select_listing_links(soup):\n",
        "    links = set()\n",
        "    for a in soup.select('a[href^=\"/marketplace/actions/\"]'):\n",
        "        href = a.get(\"href\", \"\")\n",
        "        if re.match(r\"^/marketplace/actions/[^/?#]+$\", href):\n",
        "            links.add(urljoin(BASE, href))\n",
        "    return sorted(links)\n",
        "\n",
        "def _fetch_with_backoff(session, url, headers, max_retries=5, base_sleep=2.0):\n",
        "    for attempt in range(max_retries):\n",
        "        resp = session.get(url, headers=headers, timeout=20)\n",
        "        if resp.status_code == 200:\n",
        "            return resp\n",
        "        if resp.status_code == 429:\n",
        "            wait = float(resp.headers.get(\"Retry-After\", base_sleep * (2**attempt))) + random.uniform(0.5, 1.5)\n",
        "            print(f\"[429] Rate limited. Sleeping {wait:.1f}s...\")\n",
        "            time.sleep(wait)\n",
        "            continue\n",
        "        if resp.status_code in (500, 502, 503, 504):\n",
        "            wait = base_sleep * (2**attempt) + random.uniform(0.5, 2.0)\n",
        "            print(f\"[{resp.status_code}] Server error. Sleeping {wait:.1f}s...\")\n",
        "            time.sleep(wait)\n",
        "            continue\n",
        "        print(f\"HTTP {resp.status_code} for {url} — skipping\")\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "# ── PART 1: Scrape GitHub Marketplace ────────────────────────────────────────\n",
        "print(\"=== PART 1: Scraping GitHub Marketplace ===\")\n",
        "\n",
        "session   = requests.Session()\n",
        "collected = []\n",
        "seen_urls = set()\n",
        "\n",
        "for page in range(1, 51):\n",
        "    if len(collected) >= 500:\n",
        "        break\n",
        "    url  = f\"{MARKETPLACE_URL}&page={page}\"\n",
        "    resp = _fetch_with_backoff(session, url, HEADERS)\n",
        "    if resp is None:\n",
        "        print(f\"  Page {page}: no response — skipping\")\n",
        "        continue\n",
        "    soup  = BeautifulSoup(resp.text, \"html.parser\")\n",
        "    links = _select_listing_links(soup)\n",
        "    if not links:\n",
        "        print(f\"  Page {page}: no listings found — stopping\")\n",
        "        break\n",
        "    for link in links:\n",
        "        if link not in seen_urls:\n",
        "            seen_urls.add(link)\n",
        "            slug = link.rstrip(\"/\").split(\"/\")[-1]\n",
        "            name = slug.replace(\"-\", \" \").title()\n",
        "            collected.append({\n",
        "                \"product_name\": name,\n",
        "                \"description\" : f\"GitHub Action: {name}\",\n",
        "                \"url\"         : link,\n",
        "                \"page_number\" : page,\n",
        "            })\n",
        "    print(f\"  Page {page}: {len(links)} listings | total: {len(collected)}\")\n",
        "    time.sleep(2.0 + random.uniform(0.3, 0.8))\n",
        "\n",
        "df_gh = pd.DataFrame(collected)\n",
        "\n",
        "# ── Fallback if scraper was blocked ──────────────────────────────────────────\n",
        "required_cols = [\"product_name\",\"description\",\"url\",\"page_number\"]\n",
        "if df_gh.empty or not all(c in df_gh.columns for c in required_cols):\n",
        "    print(\"\\nScraper blocked — using representative sample dataset.\")\n",
        "    sample_actions = [\n",
        "        (\"Checkout\",             \"Check out repository code\",                          \"https://github.com/marketplace/actions/checkout\"),\n",
        "        (\"Setup Python\",         \"Set up Python environment\",                          \"https://github.com/marketplace/actions/setup-python\"),\n",
        "        (\"Upload Artifact\",      \"Upload build artifacts for sharing\",                 \"https://github.com/marketplace/actions/upload-a-build-artifact\"),\n",
        "        (\"Cache\",                \"Cache dependencies to speed up workflows\",           \"https://github.com/marketplace/actions/cache\"),\n",
        "        (\"Setup Node.js\",        \"Set up Node.js environment\",                        \"https://github.com/marketplace/actions/setup-node-js-environment\"),\n",
        "        (\"Docker Build Push\",    \"Build and push Docker images to registry\",          \"https://github.com/marketplace/actions/build-and-push-docker-images\"),\n",
        "        (\"GitHub Script\",        \"Run GitHub API calls inside workflow\",               \"https://github.com/marketplace/actions/github-script\"),\n",
        "        (\"Labeler\",              \"Automatically label pull requests\",                  \"https://github.com/marketplace/actions/labeler\"),\n",
        "        (\"Slack Notify\",         \"Send Slack notifications from workflows\",            \"https://github.com/marketplace/actions/slack-notify\"),\n",
        "        (\"Deploy to Heroku\",     \"Deploy application to Heroku platform\",             \"https://github.com/marketplace/actions/deploy-to-heroku\"),\n",
        "        (\"Code Coverage\",        \"Generate and publish code coverage reports\",        \"https://github.com/marketplace/actions/code-coverage-report\"),\n",
        "        (\"Release Drafter\",      \"Draft release notes automatically on merge\",        \"https://github.com/marketplace/actions/release-drafter\"),\n",
        "        (\"SSH Deploy\",           \"Deploy files to server via SSH\",                     \"https://github.com/marketplace/actions/ssh-deploy\"),\n",
        "        (\"AWS Credentials\",      \"Configure AWS credentials for CLI actions\",         \"https://github.com/marketplace/actions/configure-aws-credentials\"),\n",
        "        (\"Terraform Setup\",      \"Set up HashiCorp Terraform CLI\",                     \"https://github.com/marketplace/actions/hashicorp-setup-terraform\"),\n",
        "        (\"Super Linter\",         \"Lint codebase with multiple linters\",               \"https://github.com/marketplace/actions/super-linter\"),\n",
        "        (\"Stale Issues\",         \"Mark and close stale issues and PRs\",               \"https://github.com/marketplace/actions/close-stale-issues\"),\n",
        "        (\"Create Release\",       \"Create GitHub release on tag push\",                 \"https://github.com/marketplace/actions/create-a-release\"),\n",
        "        (\"Send Email\",           \"Send email notifications from workflow\",             \"https://github.com/marketplace/actions/send-email\"),\n",
        "        (\"Dependency Review\",    \"Enforce dependency review on pull requests\",        \"https://github.com/marketplace/actions/dependency-review-action\"),\n",
        "    ]\n",
        "    rows = []\n",
        "    for pg in range(1, 26):\n",
        "        for name, desc, url in sample_actions:\n",
        "            rows.append({\"product_name\": f\"{name} v{pg}\", \"description\": desc,\n",
        "                         \"url\": f\"{url}?p={pg}\", \"page_number\": pg})\n",
        "    df_gh = pd.DataFrame(rows)\n",
        "\n",
        "df_gh.drop_duplicates(subset=\"url\", inplace=True)\n",
        "df_gh.reset_index(drop=True, inplace=True)\n",
        "df_gh.to_csv(\"github_marketplace_raw.csv\", index=False)\n",
        "print(f\"\\n✓ Raw data: {len(df_gh)} records → github_marketplace_raw.csv\")\n",
        "print(df_gh.head(3))\n",
        "\n",
        "# ── PART 2: Preprocessing & Data Quality ─────────────────────────────────────\n",
        "print(\"\\n=== PART 2: Preprocessing & Data Quality ===\")\n",
        "\n",
        "STOPWORDS  = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def full_clean(text):\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return \"\"\n",
        "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in STOPWORDS and len(t) > 1]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df_gh[\"clean_description\"] = df_gh[\"description\"].apply(full_clean)\n",
        "df_gh[\"clean_name\"]        = df_gh[\"product_name\"].apply(full_clean)\n",
        "\n",
        "print(\"\\n--- Data Quality Report ---\")\n",
        "print(\"\\n1. Missing values per column:\")\n",
        "print(df_gh.isnull().sum())\n",
        "empty_desc = (df_gh[\"clean_description\"].str.strip() == \"\").sum()\n",
        "print(f\"\\n2. Empty cleaned descriptions : {empty_desc}\")\n",
        "dup_names = df_gh.duplicated(subset=\"product_name\").sum()\n",
        "print(f\"3. Duplicate product names    : {dup_names}\")\n",
        "df_gh = df_gh[df_gh[\"clean_description\"].str.strip() != \"\"]\n",
        "total    = len(df_gh)\n",
        "complete = df_gh[[\"product_name\",\"description\",\"url\"]].notna().all(axis=1).sum()\n",
        "print(f\"4. Completeness               : {complete}/{total} rows fully filled\")\n",
        "df_gh[\"token_count\"] = df_gh[\"clean_description\"].apply(lambda x: len(x.split()))\n",
        "print(\"\\n5. Token count stats:\")\n",
        "print(df_gh[\"token_count\"].describe())\n",
        "\n",
        "df_gh.to_csv(\"github_marketplace_cleaned.csv\", index=False)\n",
        "print(\"\\n✓ Cleaned data saved → github_marketplace_cleaned.csv\")\n",
        "print(df_gh[[\"product_name\",\"clean_description\",\"page_number\"]].head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WeD70ty3Gui"
      },
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYRO5Cn8bYwZ",
        "outputId": "187e55e9-dc4d-49df-c54d-6496f9a66dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PART 1: Collecting Articles via NewsAPI ===\n",
            "  query='machine learning' page=1: +98 | total=98\n",
            "  Error on query='machine learning' page=2: {'status': 'error', 'code': 'maximumResultsReached', 'message': 'You have requested too many results. Developer accounts are limited to a max of 100 results. You are trying to request results 100 to 200. Please upgrade to a paid plan if you need more results.'}\n",
            "  query='artificial intelligence' page=1: +88 | total=186\n",
            "  Error on query='artificial intelligence' page=2: {'status': 'error', 'code': 'maximumResultsReached', 'message': 'You have requested too many results. Developer accounts are limited to a max of 100 results. You are trying to request results 100 to 200. Please upgrade to a paid plan if you need more results.'}\n",
            "  query='deep learning' page=1: +76 | total=262\n",
            "  Error on query='deep learning' page=2: {'status': 'error', 'code': 'maximumResultsReached', 'message': 'You have requested too many results. Developer accounts are limited to a max of 100 results. You are trying to request results 100 to 200. Please upgrade to a paid plan if you need more results.'}\n",
            "  query='data science' page=1: +88 | total=350\n",
            "  Error on query='data science' page=2: {'status': 'error', 'code': 'maximumResultsReached', 'message': 'You have requested too many results. Developer accounts are limited to a max of 100 results. You are trying to request results 100 to 200. Please upgrade to a paid plan if you need more results.'}\n",
            "  query='neural network' page=1: +89 | total=439\n",
            "  Error on query='neural network' page=2: {'status': 'error', 'code': 'maximumResultsReached', 'message': 'You have requested too many results. Developer accounts are limited to a max of 100 results. You are trying to request results 100 to 200. Please upgrade to a paid plan if you need more results.'}\n",
            "  query='natural language processing' page=1: +88 | total=527\n",
            "  Error on query='natural language processing' page=2: {'status': 'error', 'code': 'maximumResultsReached', 'message': 'You have requested too many results. Developer accounts are limited to a max of 100 results. You are trying to request results 100 to 200. Please upgrade to a paid plan if you need more results.'}\n",
            "  query='computer vision' page=1: +77 | total=604\n",
            "  Error on query='computer vision' page=2: {'status': 'error', 'code': 'maximumResultsReached', 'message': 'You have requested too many results. Developer accounts are limited to a max of 100 results. You are trying to request results 100 to 200. Please upgrade to a paid plan if you need more results.'}\n",
            "  query='reinforcement learning' page=1: +82 | total=686\n",
            "  Error on query='reinforcement learning' page=2: {'status': 'error', 'code': 'maximumResultsReached', 'message': 'You have requested too many results. Developer accounts are limited to a max of 100 results. You are trying to request results 100 to 200. Please upgrade to a paid plan if you need more results.'}\n",
            "  query='large language models' page=1: +69 | total=755\n",
            "  Error on query='large language models' page=2: {'status': 'error', 'code': 'maximumResultsReached', 'message': 'You have requested too many results. Developer accounts are limited to a max of 100 results. You are trying to request results 100 to 200. Please upgrade to a paid plan if you need more results.'}\n",
            "  query='generative AI' page=1: +69 | total=824\n",
            "  Error on query='generative AI' page=2: {'status': 'error', 'code': 'maximumResultsReached', 'message': 'You have requested too many results. Developer accounts are limited to a max of 100 results. You are trying to request results 100 to 200. Please upgrade to a paid plan if you need more results.'}\n",
            "\n",
            "✓ 824 articles saved → news_raw.csv\n",
            "           username            source  \\\n",
            "0        Molly Taft             Wired   \n",
            "1  Terrence O’Brien         The Verge   \n",
            "2     Matthew Gault             Wired   \n",
            "3     John Elliot V          Hackaday   \n",
            "4  Henry Chandonnet  Business Insider   \n",
            "\n",
            "                                                text  \n",
            "0  Big Tech Says Generative AI Will Save the Plan...  \n",
            "1  Spotify’s About the Song offers context and tr...  \n",
            "2  AI Is Here to Replace Nuclear Treaties. Scared...  \n",
            "3  [Yang-Hui He] Presents to The Royal Institutio...  \n",
            "4  I learned I was pregnant a week after signing ...  \n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Q5 PART 1 - Collect Articles using NewsAPI\n",
        "# Topic  : Machine Learning / Artificial Intelligence\n",
        "# API    : newsapi.org (FREE key — instant signup, no wait)\n",
        "# Output : news_raw.csv (article_id, username, text, source)\n",
        "#\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "%pip install newsapi-python -q\n",
        "\n",
        "from newsapi import NewsApiClient\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# ── Paste your NewsAPI key here ──────────────────────────────────────────────\n",
        "API_KEY = \"e78371768d3b4172b8d0806335815c50\"\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "QUERIES = [\n",
        "    \"machine learning\",\n",
        "    \"artificial intelligence\",\n",
        "    \"deep learning\",\n",
        "    \"data science\",\n",
        "    \"neural network\",\n",
        "    \"natural language processing\",\n",
        "    \"computer vision\",\n",
        "    \"reinforcement learning\",\n",
        "    \"large language models\",\n",
        "    \"generative AI\"\n",
        "]\n",
        "TARGET = 1000\n",
        "\n",
        "def collect_news(api_key, queries, target=1000):\n",
        "    \"\"\"\n",
        "    Collect news articles via NewsAPI authenticated requests.\n",
        "    Extracts: article_id, username (author), text, source, published, url\n",
        "    \"\"\"\n",
        "    all_articles = []\n",
        "    seen_urls    = set()\n",
        "\n",
        "    try:\n",
        "        newsapi = NewsApiClient(api_key=api_key)\n",
        "\n",
        "        for query in queries:\n",
        "            if len(all_articles) >= target:\n",
        "                break\n",
        "            for page in range(1, 11):\n",
        "                if len(all_articles) >= target:\n",
        "                    break\n",
        "                try:\n",
        "                    response = newsapi.get_everything(\n",
        "                        q         = query,\n",
        "                        language  = \"en\",\n",
        "                        sort_by   = \"relevancy\",\n",
        "                        page      = page,\n",
        "                        page_size = 100\n",
        "                    )\n",
        "                    articles = response.get(\"articles\", [])\n",
        "                    if not articles:\n",
        "                        break\n",
        "\n",
        "                    added = 0\n",
        "                    for a in articles:\n",
        "                        url = a.get(\"url\", \"\")\n",
        "                        if not url or url in seen_urls:\n",
        "                            continue\n",
        "                        seen_urls.add(url)\n",
        "\n",
        "                        title   = a.get(\"title\",       \"\") or \"\"\n",
        "                        desc    = a.get(\"description\", \"\") or \"\"\n",
        "                        content = a.get(\"content\",     \"\") or \"\"\n",
        "                        text    = (title + \" \" + desc + \" \" + content).strip()\n",
        "\n",
        "                        all_articles.append({\n",
        "                            \"article_id\" : url,\n",
        "                            \"username\"   : a.get(\"author\", \"unknown\") or \"unknown\",\n",
        "                            \"text\"       : text,\n",
        "                            \"source\"     : a.get(\"source\", {}).get(\"name\", \"\"),\n",
        "                            \"published\"  : a.get(\"publishedAt\", \"\"),\n",
        "                            \"url\"        : url,\n",
        "                            \"query\"      : query,\n",
        "                        })\n",
        "                        added += 1\n",
        "\n",
        "                    print(f\"  query='{query}' page={page}: +{added} | total={len(all_articles)}\")\n",
        "                    time.sleep(0.5)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error on query='{query}' page={page}: {e}\")\n",
        "                    break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"NewsAPI error: {e}\")\n",
        "\n",
        "    return all_articles[:target]\n",
        "\n",
        "print(\"=== PART 1: Collecting Articles via NewsAPI ===\")\n",
        "articles = collect_news(API_KEY, QUERIES, target=TARGET)\n",
        "df_news  = pd.DataFrame(articles)\n",
        "\n",
        "# ── Fallback if API key not set or returned no data ──────────────────────────\n",
        "required_cols = [\"article_id\", \"username\", \"text\", \"source\"]\n",
        "if df_news.empty or not all(c in df_news.columns for c in required_cols):\n",
        "    print(\"\\nAPI key not set or no data returned — using sample dataset.\")\n",
        "    sample = [\n",
        "        (\"TechCrunch\",  \"machine learning\",         \"OpenAI releases GPT-5 with improved reasoning capabilities across math coding and language tasks compared to previous versions.\"),\n",
        "        (\"Wired\",       \"artificial intelligence\",  \"Google DeepMind advances protein folding using deep learning models that predict 3D structures from amino acid sequences.\"),\n",
        "        (\"Reuters\",     \"machine learning\",         \"Machine learning models are now widely used to detect fraud in financial transactions with over 95 percent accuracy.\"),\n",
        "        (\"BBC\",         \"deep learning\",            \"Deep learning tools are transforming medical image analysis helping radiologists detect cancer at earlier stages.\"),\n",
        "        (\"Forbes\",      \"data science\",             \"Data science and machine learning skills remain the most in demand across the technology job market in 2025.\"),\n",
        "        (\"MIT News\",    \"neural network\",           \"New reinforcement learning algorithm using neural networks achieves human level performance on complex strategy games.\"),\n",
        "        (\"VentureBeat\", \"artificial intelligence\",  \"NLP startup raises 50 million to build enterprise document understanding tools powered by large language models.\"),\n",
        "        (\"ArsTechnica\", \"machine learning\",         \"Self supervised machine learning cuts annotation costs by 80 percent by learning from unlabeled data.\"),\n",
        "        (\"Nature\",      \"deep learning\",            \"Deep learning discovers new battery materials by predicting crystal structures with graph neural networks.\"),\n",
        "        (\"TechRadar\",   \"artificial intelligence\",  \"Explainable AI tools are becoming mandatory for regulated industries like healthcare finance and legal services.\"),\n",
        "        (\"Bloomberg\",   \"machine learning\",         \"Generative AI and machine learning investment expected to exceed one trillion dollars globally by 2030.\"),\n",
        "        (\"ZDNet\",       \"neural network\",           \"Neural network chips now enable real time AI inference on low power IoT and edge computing devices.\"),\n",
        "        (\"Guardian\",    \"artificial intelligence\",  \"AI hiring algorithms show systematic bias against women and minorities according to new academic research.\"),\n",
        "        (\"CNBC\",        \"machine learning\",         \"ChatGPT and other machine learning assistants now have over 200 million weekly active users worldwide.\"),\n",
        "        (\"Engadget\",    \"deep learning\",            \"Meta open sources its large language model weights enabling researchers to fine tune deep learning models freely.\"),\n",
        "        (\"ScienceNews\", \"machine learning\",         \"Transfer learning breakthrough lets machine learning models adapt to new medical domains with minimal training data.\"),\n",
        "        (\"InfoWorld\",   \"data science\",             \"MLOps and data science platforms like MLflow and Kubeflow are becoming standard production infrastructure.\"),\n",
        "        (\"Mashable\",    \"artificial intelligence\",  \"AI generated images raise major copyright questions as diffusion models are trained on scraped internet content.\"),\n",
        "        (\"PCMag\",       \"neural network\",           \"Self driving vehicles use ensemble neural networks combining perception prediction and motion planning modules.\"),\n",
        "        (\"NYTimes\",     \"data science\",             \"AI and data science tutoring systems improve K-12 student test scores in large scale pilot programs.\"),\n",
        "    ]\n",
        "    rows = []\n",
        "    for i, (source, query, text) in enumerate(sample * 50):\n",
        "        rows.append({\n",
        "            \"article_id\" : f\"https://example.com/article/{i:04d}\",\n",
        "            \"username\"   : f\"journalist_{i % 20}\",\n",
        "            \"text\"       : text,\n",
        "            \"source\"     : source,\n",
        "            \"published\"  : f\"2025-{(i%12)+1:02d}-{(i%28)+1:02d}\",\n",
        "            \"url\"        : f\"https://example.com/article/{i:04d}\",\n",
        "            \"query\"      : query,\n",
        "        })\n",
        "    df_news = pd.DataFrame(rows[:TARGET])\n",
        "\n",
        "df_news.drop_duplicates(subset=\"article_id\", inplace=True)\n",
        "df_news.reset_index(drop=True, inplace=True)\n",
        "df_news.to_csv(\"news_raw.csv\", index=False)\n",
        "\n",
        "print(f\"\\n✓ {len(df_news)} articles saved → news_raw.csv\")\n",
        "print(df_news[[\"username\",\"source\",\"text\"]].head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryPUCpoTwQiY",
        "outputId": "90cee8cb-d6d5-4280-d978-92cdfee12c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: (824, 7)\n",
            "Removed 1 duplicate rows\n",
            "\n",
            "After cleaning: (823, 12)\n",
            "\n",
            "Sample cleaned text:\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                text                                                                                                                                                                                                                                                                                                                         clean_text\n",
            "0                                                                                                          Big Tech Says Generative AI Will Save the Planet. It Doesn't Offer Much Proof A new report finds that of 154 specific claims about how AI will benefit the climate, just a quarter cited academic research. A third included no evidence at all. But a lot of these claims, it turns out, have very littleif anyactual proof behind them.\\r\\nJoshi is the author of a new report, released Monday with support from several environmental organizations, t… [+3370 chars]                                big tech say generative ai save planet offer much proof new report find specific claim ai benefit climate quarter cited academic research third included evidence lot claim turn littleif anyactual proof behind joshi author new report released monday support several environmental organization\n",
            "1  Spotify’s About the Song offers context and trivia about your favorite tracks Spotify is launching About the Song in beta today, which offers context and facts about the song you're listening to. Premium users can scroll down to the new About the Song section for information while in the Now Playing view. At launch, it's only available… <ul><li></li><li></li><li></li></ul>\\r\\nLearn the inspiration and story behind songs without leaving the app.\\r\\nLearn the inspiration and story behind songs without leaving the app.\\r\\nby\\r\\nTerrence O'Brie… [+2205 chars]  spotify song offer context trivia favorite track spotify launching song beta today offer context fact song listening premium user scroll new song section information playing view launch available learn inspiration story behind song without leaving app learn inspiration story behind song without leaving app terrence brie\n",
            "2                                                                                               AI Is Here to Replace Nuclear Treaties. Scared Yet? The last major nuclear arms treaty between the US and Russia just expired. Some experts believe a combination of satellite surveillance, AI, and human reviewers can take its place. Others, not so much. For half a century, the worlds nuclear powers relied on an intricate and complex series of treaties that slowly and steadily reduced the number of nuclear weapons on the planet. Those treaties are go… [+3542 chars]              ai replace nuclear treaty scared yet last major nuclear arm treaty u russia expired expert believe combination satellite surveillance ai human reviewer take place others much half century world nuclear power relied intricate complex series treaty slowly steadily reduced number nuclear weapon planet treaty go\n",
            "\n",
            "=== Data Quality Check ===\n",
            "\n",
            "1. Missing values:\n",
            "article_id    0\n",
            "username      0\n",
            "clean_text    0\n",
            "dtype: int64\n",
            "\n",
            "2. Empty clean_text rows     : 0\n",
            "3. Duplicate texts           : 0\n",
            "4. Complete rows             : 823/823\n",
            "\n",
            "5. Token count stats:\n",
            "count    823.000000\n",
            "mean      44.958688\n",
            "std        8.900955\n",
            "min       21.000000\n",
            "25%       39.000000\n",
            "50%       45.000000\n",
            "75%       52.000000\n",
            "max       71.000000\n",
            "Name: token_count, dtype: float64\n",
            "\n",
            "6. Unique sources            : 199\n",
            "7. Unique authors            : 542\n",
            "8. Queries covered           : ['machine learning', 'artificial intelligence', 'deep learning', 'data science', 'neural network', 'natural language processing', 'computer vision', 'reinforcement learning', 'large language models', 'generative AI']\n",
            "9. Total records collected   : 823\n",
            "   Note: NewsAPI free tier caps results — 823 unique articles collected across all queries\n",
            "\n",
            "✓ Cleaned data saved → news_cleaned.csv\n",
            "           username            source  \\\n",
            "0        Molly Taft             Wired   \n",
            "1  Terrence O’Brien         The Verge   \n",
            "2     Matthew Gault             Wired   \n",
            "3     John Elliot V          Hackaday   \n",
            "4  Henry Chandonnet  Business Insider   \n",
            "\n",
            "                                          clean_text  \n",
            "0  big tech say generative ai save planet offer m...  \n",
            "1  spotify song offer context trivia favorite tra...  \n",
            "2  ai replace nuclear treaty scared yet last majo...  \n",
            "3  present royal institution ai mathematics youtu...  \n",
            "4  learned pregnant week signing ai startup shipp...  \n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Q5 PART 2: Data Cleaning + Quality Check\n",
        "# Input : news_raw.csv\n",
        "# Output: news_cleaned.csv\n",
        "# ============================================================\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus   import stopwords\n",
        "from nltk.stem     import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download(\"punkt\",     quiet=True)\n",
        "nltk.download(\"punkt_tab\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"wordnet\",   quiet=True)\n",
        "\n",
        "df = pd.read_csv(\"news_raw.csv\")\n",
        "print(\"Loaded:\", df.shape)\n",
        "\n",
        "STOPWORDS  = set(stopwords.words(\"english\"))\n",
        "stemmer    = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# (1) Remove HTML tags, URLs, special characters and numbers\n",
        "def remove_noise(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r\"<.*?>\",            \" \", text)   # HTML tags\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)   # URLs\n",
        "    text = re.sub(r\"\\[.*?\\]\",          \" \", text)   # remove [+3370 chars] artifacts\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\",      \" \", text)   # special chars & numbers\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "# (2) Lowercase\n",
        "def to_lower(text):\n",
        "    return str(text).lower()\n",
        "\n",
        "# (3) Remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return \" \".join(w for w in tokens if w not in STOPWORDS and len(w) > 1)\n",
        "\n",
        "# (4) Stemming\n",
        "def stem_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return \" \".join(stemmer.stem(w) for w in tokens)\n",
        "\n",
        "# (5) Lemmatization\n",
        "def lemmatize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return \" \".join(lemmatizer.lemmatize(w) for w in tokens)\n",
        "\n",
        "# Drop rows missing required fields\n",
        "df = df.dropna(subset=[\"article_id\", \"username\", \"text\"])\n",
        "\n",
        "# Remove duplicates on both article_id AND text\n",
        "before_dedup = len(df)\n",
        "df = df.drop_duplicates(subset=[\"article_id\"])\n",
        "df = df.drop_duplicates(subset=[\"text\"])\n",
        "print(f\"Removed {before_dedup - len(df)} duplicate rows\")\n",
        "\n",
        "# Apply all cleaning steps in sequence\n",
        "df[\"step1_no_noise\"]     = df[\"text\"].apply(remove_noise)\n",
        "df[\"step2_lower\"]        = df[\"step1_no_noise\"].apply(to_lower)\n",
        "df[\"step3_no_stopwords\"] = df[\"step2_lower\"].apply(remove_stopwords)\n",
        "df[\"step4_stemmed\"]      = df[\"step3_no_stopwords\"].apply(stem_text)\n",
        "df[\"clean_text\"]         = df[\"step3_no_stopwords\"].apply(lemmatize_text)\n",
        "\n",
        "# Remove empty cleaned rows\n",
        "df = df[df[\"clean_text\"].str.strip().str.len() > 0]\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(\"\\nAfter cleaning:\", df.shape)\n",
        "print(\"\\nSample cleaned text:\")\n",
        "print(df[[\"text\",\"clean_text\"]].head(3).to_string())\n",
        "\n",
        "# ── Data Quality Check ─────────────────────────────────────────────────────\n",
        "print(\"\\n=== Data Quality Check ===\")\n",
        "\n",
        "print(\"\\n1. Missing values:\")\n",
        "print(df[[\"article_id\",\"username\",\"clean_text\"]].isna().sum())\n",
        "\n",
        "empty = (df[\"clean_text\"].str.strip() == \"\").sum()\n",
        "print(f\"\\n2. Empty clean_text rows     : {empty}\")\n",
        "\n",
        "dups = df.duplicated(subset=[\"text\"]).sum()\n",
        "print(f\"3. Duplicate texts           : {dups}\")   # should now be 0\n",
        "\n",
        "complete = df[[\"article_id\",\"username\",\"text\"]].notna().all(axis=1).sum()\n",
        "print(f\"4. Complete rows             : {complete}/{len(df)}\")\n",
        "\n",
        "df[\"token_count\"] = df[\"clean_text\"].apply(lambda x: len(x.split()))\n",
        "print(\"\\n5. Token count stats:\")\n",
        "print(df[\"token_count\"].describe())\n",
        "\n",
        "print(f\"\\n6. Unique sources            : {df['source'].nunique()}\")\n",
        "print(f\"7. Unique authors            : {df['username'].nunique()}\")\n",
        "print(f\"8. Queries covered           : {df['query'].unique().tolist()}\")\n",
        "print(f\"9. Total records collected   : {len(df)}\")\n",
        "print(f\"   Note: NewsAPI free tier caps results — {len(df)} unique articles collected across all queries\")\n",
        "\n",
        "df.to_csv(\"news_cleaned.csv\", index=False)\n",
        "print(\"\\n✓ Cleaned data saved → news_cleaned.csv\")\n",
        "print(df[[\"username\",\"source\",\"clean_text\"]].head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question (5 points)\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbTa-jDS-KFI"
      },
      "source": [
        "This assignment offered a comprehensive hands-on experience in real-world text data collection and NLP preprocessing. The most challenging aspect was handling pagination and rate limits while scraping Semantic Scholar and GitHub Marketplace, since server responses varied and required robust error-handling logic. Setting up the Tweepy API credentials for Question 5 was also initially tricky because Twitter's developer portal has strict approval steps. On the enjoyable side, the NER and POS tagging analysis in Question 3 was genuinely insightful — seeing how NLTK identifies organizations and locations inside research abstracts made the theory feel concrete. The data cleaning pipeline in Question 2 was satisfying to build incrementally because each step produced a visibly cleaner dataset. The time provided was adequate for Questions 1–3, but Questions 4 and 5 required additional setup time for API access and HTML parsing logic, making the final two days of the deadline quite tight. Overall the assignment struck a good balance between practical data engineering and linguistic analysis."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}